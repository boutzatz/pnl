\section{Deterministic methods}

% --------------------------------------------------------------------- %%
% Roots
\subsection{Root finding}
\subsubsection{Short Description}

To provide a uniformed framework to root finding functions, we use several
structures for storing different kind of functions. The pointer
\var{params} is used to store the extra parameters. These new types come
with dedicated macros starting in \verb!PNL_EVAL!  to evaluate the function
and their Jacobian.
\begin{verbatim}
/*
 * f: R --> R
 * The function  pointer returns f(x)
 */
typedef struct {
  double (*function) (double x, void *params);
  void *params;
} PnlFunc ;
#define PNL_EVAL_FUNC(F, x) (*((F)->function))(x, (F)->params)

/*
 * f: R^2 --> R
 * The function pointer returns f(x)
 */
typedef struct {
  double (*function) (double x, double y, void *params);
  void *params;
} PnlFunc2D ;
#define PNL_EVAL_FUNC2D(F, x, y) (*((F)->function))(x, y, (F)->params)

/*
 * f: R --> R
 * The function pointer computes f(x) and Df(x) and stores them in fx
 * and dfx respectively
 */
typedef struct {
  void (*function) (double x, double *fx, double *dfx, void *params);
  void *params;
} PnlFuncDFunc ;
#define PNL_EVAL_FUNC_DFUNC(F, x, fx, dfx) (*((F)->function))(x, fx, dfx, (F)->params)

/*
 * f: R^n --> R
 * The function pointer returns f(x)
 */
typedef struct {
  double (*function) (const PnlVect *x, void *params);
  void *params;
} PnlRnFuncR ;
#define PNL_EVAL_RNFUNCR(F, x) (*((F)->function))(x, (F)->params)

/*
 * f: R^n --> R^m
 * The function pointer computes the vector f(x) and stores it in
 * fx (vector of size m)
 */
typedef struct {
  void (*function) (const PnlVect *x, PnlVect *fx, void *params);
  void *params;
} PnlRnFuncRm ;
#define PNL_EVAL_RNFUNCRM(F, x, fx) (*((F)->function))(x, fx, (F)->params)

/*
 * Synonymous of PnlRnFuncRm for f:R^n --> R^n 
 */
typedef PnlRnFuncRm PnlRnFuncRn;
#define PNL_EVAL_RNFUNCRN  PNL_EVAL_RNFUNCRM


/*
 * f: R^n --> R^m
 * The function pointer computes the vector f(x) and stores it in fx
 * (vector of size m) 
 * The Dfunction pointer computes the matrix Df(x) and stores it in dfx
 * (matrix of size m x n) 
 */
typedef struct {
  void (*function) (const PnlVect *x, PnlVect *fx, void *params);
  void (*Dfunction) (const PnlVect *x, PnlMat *dfx, void *params);
  void *params;
} PnlRnFuncRmDFunc ;
#define PNL_EVAL_RNFUNCRM_DFUNC(F, x, dfx) (*((F)->Dfunction))(x, dfx, (F)->params)

/*
 * Synonymous of PnlRnFuncRmDFunc for f:R^n --> R^m
 */
typedef PnlRnFuncRmDFunc PnlRnFuncRnDFunc;
#define PNL_EVAL_RNFUNCRN_DFUNC PNL_EVAL_RNFUNCRM_DFUNC
\end{verbatim}

\subsubsection{Functions}

To use the following functions, you should include \verb!pnl_root.h!.

For finding the zero of a real valued function we provide the following
functions.
\begin{itemize}
  \item \describefun{double}{pnl_root_brent}{\refstruct{PnlFunc}\ptr  F, double
    x1, double  x2, double \ptr tol}
    \sshortdescribe Finds the root of \var{F} between \var{x1} and \var{x2} with
    an accuracy of order \var{tol}. On exit \var{tol} is an upper bound of the
    error.

  \item \describefun{int}{pnl_find_root}{\refstruct{PnlFuncDFunc}\ptr  Func, 
    double x_min, double x_max, double tol, int N_Max, double\ptr  res}
    \sshortdescribe Finds the root of \var{F} between \var{x1} and \var{x2} with
    an accuracy of order \var{tol} and a maximum of \var{N_max} iterations. On
    exit, the root is stored in \var{res}. Note that the function \var{F} must
    also compute the first derivative of the function.


  \item \describefun{int}{pnl_root_newton}{\refstruct{PnlFuncDFunc} \ptr Func, 
    double x0, double epsrel, double epsabs, int N_max, double \ptr res}
    \sshortdescribe Finds the root of \var{F} starting from \var{x0} with an
    accuracy given both by \var{epsrel} and \var{epsabs} and a maximum number of
    iterations \var{N_max}. On exit, the root is stored in \var{res}.Note that
    the function \var{F} must also compute the first derivative of the function.

  \item \describefun{int}{pnl_root_bisection}{\refstruct{PnlFunc } \ptr Func, 
    double xmin, double xmax, double epsrel, double espabs, int N_max, double
    \ptr res}
    \sshortdescribe Finds the root of \var{F} between \var{x1} and \var{x2} with
    an accuracy given both by \var{epsrel} and \var{epsabs} and a maximum number
    of iterations \var{N_max}. On exit, the root is stored in \var{res}
\end{itemize}

Searching for the zero of a multivariate and vector valued function is a
complicated problem and we rely on minpack for doing this. Here, we provide
two wrappers for calling minpack routines.
\begin{itemize}
  \item \describefun
    {int}{pnl_root_fsolve}{\refstruct{PnlRnFuncRnDFunc} \ptr f,
    \refstruct{PnlVect} \ptr x, \refstruct{PnlVect} \ptr fx, double xtol,
    int maxfev, int \ptr nfev, \refstruct{PnlVect} \ptr scale, int
    error_msg}
    \sshortdescribe Computes the root of a function $f:\R^n \longmapsto
    \R^n$. Note that the number of components of \var{f} must be equal to the
    number of variates of \var{f}. This function returns \var{OK} or
    \var{FAIL} if something went wrong. 
    \parameters
    \begin{itemize}
      \item \var{f} is a pointer to a \refstruct{PnlRnFuncRnDFunc} used to
        store the function whose root is to be found. \var{f} can also
        store the Jacobian of the function, if not it is computed using
        finite differences (see the file \url{examples/minpack_test.c} for
        a usage example),
      \item  \var{x} contains on input the starting point of the search and
        an approximation of the root of \var{f} on output,
      \item \var{xtol} is the precision required on \var{x}, if set to 0 a
        default value is used.
      \item \var{maxfev} is the maximum number of evaluations of the function
        \var{f} before the algorithm returns, if set to 0, a coherent
        number is determined internally.
      \item \var{nfev} contains on output the number of evaluations of
        \var{f} during the algorithm,
      \item \var{scale} is a vector used to rescale \var{x} in a way that
        each coordinate of the solution is approximately of order 1 after
        rescaling. If on input \var{scale=NULL}, a scaling vector is
        computed internally by the algorithm.
      \item \var{error_msg} is a boolean
        (\var{TRUE} or \var{FALSE}) to specify if an error message should be
        printed when the algorithm stops before having converged.
      \item On output, \var{fx} contains \var{f(x)}.
    \end{itemize}

  \item \describefun {int}{pnl_root_lsq}{\refstruct{PnlRnFuncRmDFunc}
    \ptr f, \refstruct{PnlVect} \ptr x, int m, \refstruct{PnlVect} \ptr fx,
    double xtol, double ftol, double gtol, int maxfev, int \ptr nfev,
    \refstruct{PnlVect} \ptr scale, int error_msg}
    \sshortdescribe Computes the root of $x \in \R^n \longmapsto
    \sum_{i=1}^m f_i(x)^2$, note that there is no reason why \var{m} should
    be equal to \var{n}.
    \parameters
    \begin{itemize}
      \item \var{f} is a pointer to a \refstruct{PnlRnFuncRmDFunc} used to
        store the function whose root is to be found. \var{f} can also
        store the Jacobian of the function, if not it is computed using
        finite differences (see the file \url{examples/minpack_test.c} for
        a usage example),
      \item  \var{x} contains on input the starting
        point of the search and an approximation of the root of \var{f} on
        output,
      \item \var{m} is the number of components of \var{f},      
      \item \var{xtol} is the precision required on \var{x}, if set to 0 a
        default value is used.
      \item \var{ftol} is the precision required on \var{f}, if set to 0 a
        default value is used.
      \item \var{gtol} is the precision required on the Jacobian of
        \var{f}, if set to 0 a default value is used.
      \item \var{maxfev} is the maximum number of evaluations of the function
        \var{f} before the algorithm returns, if set to 0, a coherent
        number is determined internally.
      \item \var{nfev} contains on output the number of evaluations of
        \var{f} during the algorithm,
      \item \var{scale} is a vector used to rescale \var{x} in a way that
        each coordinate of the solution is approximately of order 1 after
        rescaling.  If on input \var{scale=NULL}, a scaling vector is
        computed internally by the algorithm.
      \item \var{error_msg} is a boolean (\var{TRUE} or \var{FALSE}) to
        specify if an error message should be printed when the algorithm
        stops before having converged.
      \item On output, \var{fx} contains \var{f(x)}.
    \end{itemize}
\end{itemize}

% --------------------------------------------------------------------- %
\subsection{Polynomial bases and regression}
\subsubsection{Short Description}

\begin{verbatim}
struct PnlBasis_t {
  int         id;
  const char *label; /*!< string to label the basis */
  int         nb_variates;  /*!< number of variates */
  int         nb_func; /*!< number of elements in the basis */
  PnlMatInt  *T; /*!< Tensor matrix */
  double    (*f)(double    *x, int i); /*!< Computes the i-th element 
                                        of the one dimensional basis */
  double    (*Df)(double   *x, int i); /*!< Computes the first derivative 
                   of the i-th element of the one dimensional basis */
  double    (*D2f)(double  *x, int i); /*!< Computes the second derivative
                      of the i-th element of the one dimensional basis */
};
\end{verbatim}

\begin{table}[h!]
  \begin{describeconst}
    \constentry{CANONICAL}{for the Canonical polynomials}
    \constentry{HERMITIAN}{for the Hermite polynomials}
    \constentry{TCHEBYCHEV}{for the Tchebychev polynomials}
  \end{describeconst}
  \caption{Names of the bases}
  \label{basis_index}
\end{table}

In this section, we provide functions to solve regression problems on
polynomial functions. Let $(x_i, i=1 \dots n)$ be $n$ points in $\R^d$ and a
function $g$ defined by the data $(y_i = g(x_i), i=1 \dots n)$. Assume you
want to approximate the function $g$ by its decomposition on a family of $N$
polynomial functions $(f_j, j=1\dots N)$. Then, we want to compute the vector
$\alpha^\star \in \R^N$ which solves
\begin{equation*} \alpha^\star = \arg\min_\alpha \sum_{i=1}^{n}
  \left(\sum_{j=0}^N \alpha_j f_j(x_i) - y_i\right)^2
\end{equation*}

\subsubsection{Functions}

\begin{itemize}
  \item \describefun{\refstruct{PnlBasis} *}{pnl_basis_new}{}
    \sshortdescribe Creates an empty \refstruct{PnlBasis}.

  \item \describefun{\refstruct{PnlBasis} *}{pnl_basis_create}{int index, int
    nb_func, int nb_variates}
    \sshortdescribe Creates a \refstruct{PnlBasis} for the polynomial family
    defined by \var{index} (see Table~\ref{basis_index}) with \var{nb_variates}
    variates. The basis will contain \var{nb_func}. 

  \item \describefun{\refstruct{PnlBasis} *}{pnl_basis_create_from_degree}{int
    index, int degree, int nb_variates}
    \sshortdescribe Creates a \refstruct{PnlBasis} for the polynomial family
    defined by \var{index} (see Table~\ref{basis_index}) with total degree less
    or equal than \var{degree} and \var{nb_variates} variates.\\
    For instance, calling \verb!pnl_basis_create_from_degree (index, 2, 4)! is
    equivalent to calling \verb!pnl_basis_create_from_tensor (index, T)! where
    \var{T} is given by
    \[ \left(
    \begin{array}{cccc}
      0 & 0 & 0 & 0\\
      1 & 0 & 0 & 0\\
      0 & 1 & 0 & 0\\
      0 & 0 & 1 & 0\\
      0 & 0 & 0 & 1\\
      1 & 1 & 0 & 0\\
      1 & 0 & 1 & 0\\
      1 & 0 & 0 & 1\\
      0 & 1 & 1 & 0\\
      0 & 1 & 0 & 1\\
      0 & 0 & 1 & 1\\
      2 & 0 & 0 & 0\\
      0 & 2 & 0 & 0\\
      0 & 0 & 2 & 0\\
      0 & 0 & 0 & 2\\
    \end{array}
    \right) \]


  \item \describefun{\refstruct{PnlBasis} *}{pnl_basis_create_from_tensor}{int
    index, PnlMatInt \ptr T}
    \sshortdescribe Creates a \refstruct{PnlBasis} for the polynomial family
    defined by \var{index} (see Table~\ref{basis_index}) using the basis
    described by the tensor matrix \var{T}. The number of lines of \var{T} is
    the number of functions of the basis whereas the numbers of columns of
    \var{T} is the number of variates of the functions.
    Note that \var{T} is not copied inside this function but only its address is
    stored, so {\bf never} free \var{T}. It will be freed when calling
    \reffun{pnl_basis_free} on the returned object. i\\
    Here is an example of a tensor matrix. Assume you are working with three
    variate functions, the basis \verb!{ 1, x, y, z, x^2, xy, yz, z^3}! is
    decomposed in the one dimensional canonical basis using the following tensor
    matrix
    \[ \left(
    \begin{array}{ccc}
      0 & 0 & 0 \\
      1 & 0 & 0 \\
      0 & 1 & 0 \\
      0 & 0 & 1 \\
      2 & 0 & 0 \\
      1 & 1 & 0 \\
      0 & 1 & 1\\
      0 & 0 & 3
    \end{array}
    \right) \]
    
  \item  \describefun{void}{pnl_basis_free}{\refstruct{PnlBasis} \ptr\ptr basis}
    \sshortdescribe Frees a \refstruct{PnlBasis} created by
    \reffun{pnl_basis_create}. Beware that \var{basis} is the address of a
    \refstruct{PnlBasis}\ptr.

  \item  \describefun{void}{pnl_basis_i}{\refstruct{PnlBasis }\ptr basis,
    double \ptr x, int i}
    \sshortdescribe If \var{basis} is composed of $f_0, \dots, f_{\var{nb\_func}}$,
    then this function returns $f_i(x)$. \var{x} must be a C array of length
    \var{nb_variates}.

  \item \describefun{int}{pnl_basis_fit_ls}{\refstruct{PnlBasis} \ptr
    P, \refstruct{PnlVect} \ptr  coef, \refstruct{PnlMat} \ptr  x,
    \refstruct{PnlVect} \ptr  y}
    \sshortdescribe Computes the coefficients \var{coef} defined by
    \begin{equation*}
      \var{coef} = \arg\min_\alpha \sum_{i=1}^n
      \left( \sum_{j=0}^{\var{N}} \alpha_j  P_j(x_i) - y_i\right)^2
    \end{equation*}
    where \var{N} is the number of functions to regress upon and $n$ is the
    number of points at which we know the value of the original function. $P_j$
    is the $j-th$ basis function. Each row of the matrix \var{x} defines the
    coordinates of one point $x_i$. The function to be approximated is defined
    by the data \var{y} which is the vector of the values taken by the function
    at the points \var{x}.


  \item \describefun{double}{pnl_basis_eval}{\refstruct{PnlBasis} \ptr
    P, \refstruct{PnlVect}\ptr  coef, double \ptr x}
    \sshortdescribe Computes the linear combination of \var{P_k(x)} defined by
    \var{coef}. Given the coefficients computed by the function
    \reffun{pnl_basis_fit_ls}, this function returns $\sum_{k=0}^n
    \var{coef}_k  P_k(\var{x})$ where \var{x} is a C array of length
    \var{nb_variates}.

  \item \describefun{double}{pnl_basis_eval_D}{\refstruct{PnlBasis} \ptr
    P, \refstruct{PnlVect} \ptr  coef, double \ptr x, int i}
    \sshortdescribe Computes the derivative with respect to \var{x_i} of the
    linear combination of \var{P_k(x)} defined by \var{coef}. Given the
    coefficients computed by the function \reffun{pnl_basis_fit_ls}, this
    function returns $\partial_{x_i} \sum_{k=0}^n \var{coef}_k  P_k(\var{x})$
    where \var{x} is a C array of length \var{nb_variates}. The index \var{i}
    may vary between \var{0} and \var{P->nb_variates - 1}.


  \item \describefun{double}{pnl_basis_eval_D2}{\refstruct{PnlBasis} \ptr  P,
    \refstruct{PnlVect} \ptr  coef, double \ptr x,  int i, int j}
    \sshortdescribe Computes the derivative with respect to \var{x_i} of the
    linear combination of \var{P_k(x)} defined by \var{coef}. Given the
    coefficients computed by the function \reffun{pnl_basis_fit_ls}, this
    function returns $\partial_{x_i} \partial_{x_j} \sum_{k=0}^n \var{coef}_k  P_k(\var{x})$
    where \var{x} is a C array of length \var{nb_variates}. The indices \var{i}
    and \var{j} may vary between \var{0} and \var{P->nb_variates - 1}.
\end{itemize}


\subsection{Numerical integration}
\subsubsection{Short Description}

To use these functionalities, you should include \verb!pnl_integration.h!.

Numerical integration methods are designed to numerically evaluate the integral
over a finite or non finite interval (resp. over a square) of real valued
functions defined on $\R$ (resp. on $\R^2$).

\begin{verbatim}
typedef struct {
  double (*function) (double x, void *params);
  void *params;
} PnlFunc;

typedef struct {
  double (*function) (double x, double y, void *params);
  void *params;
} PnlFunc2D;
\end{verbatim}

We provide the following two macros to evaluate a \refstruct{PnlFunc} or
\refstruct{PnlFunc2D} at a given point
\begin{verbatim}
#define PNL_EVAL_FUNC(F, x) (*((F)->function))(x, (F)->params)
#define PNL_EVAL_FUNC2D(F, x, y) (*((F)->function))(x, y, (F)->params)
\end{verbatim}



\subsubsection{Functions}

\begin{itemize}
\item \describefun{double}{pnl_integration}{\refstruct{PnlFunc} \ptr F, 
    double x0, double x1, int n, char \ptr meth}
  \sshortdescribe Evaluates $\int_{x_0}^{x_1} F$ using \var{n} discretization
  steps. The method used to discretize the integral is defined by \var{meth}
  which can be \var{"rect"} (rectangle rule), \var{"trap"} (trapezoidal rule),
  \var{"simpson"} (Simpson's rule).

\item \describefun{double}{pnl_integration_2d}{\refstruct{PnlFunc2D} \ptr F,
    double x0, double x1, double y0, double y1, int nx, int ny, char \ptr meth}
  \sshortdescribe Evaluates $\int_{[x_0, x_1] \times [y_0, y_1]} F$ using
  \var{nx} (resp. \var{ny}) discretization steps for \var{[x0, x1]}
  (resp. \var{[y0, y1]}). The method used to discretize the integral is
  defined by \var{meth} which can be \var{"rect"} (rectangle rule),
  \var{"trap"} (trapezoidal rule),   \var{"simpson"} (Simpson's rule).


\item \describefun{int}{pnl_integration_qng}{\refstruct{PnlFunc} \ptr F, 
    double x0, double x1, double epsabs, double epsrel, double \ptr result, 
    double \ptr abserr,  int \ptr neval}
  \sshortdescribe Evaluates $\int_{x_0}^{x_1} F$ with an absolute error less
  than \var{espabs} and a relative error less than \var{esprel}. The value of
  the integral is stored in \var{result}, while the variables \var{abserr} and
  \var{neval} respectively contain the absolute error and the number of function
  evaluations. This function returns \var{OK} if the required accuracy has been
  reached and \var{FAIL} otherwise. This function uses a non-adaptive Gauss
  Konrod procedure (qng routine from {\it QuadPack}).

\item \describefun{int}{pnl_integration_GK}{\refstruct{PnlFunc} \ptr F, 
    double x0, double x1, double epsabs, double epsrel, double \ptr result, 
    double \ptr abserr,  int \ptr neval}
    \sshortdescribe This function is a synonymous of
    \reffun{pnl_integration_qng} and is only available for backward
    compatibility. It is deprecated, please use \reffun{pnl_integration_qng}
    instead. 

\item \describefun{int}{pnl_integration_qng_2d}{\refstruct{PnlFunc2D} \ptr F, 
    double x0, double x1, double y0, double y1, double epsabs, double epsrel, 
    double \ptr result, double \ptr abserr, int \ptr neval}
  \sshortdescribe Evaluates $\int_{[x_0, x_1] \times [y_0, y_1]} F$ with an
  absolute error less than \var{espabs} and a relative error less than
  \var{esprel}. The value of the integral is stored in \var{result}, while the
  variables \var{abserr} and \var{neval} respectively contain the absolute error
  and the number of function evaluations. This function returns \var{OK} if the
  required accuracy has been reached and \var{FAIL} otherwise.

\item \describefun{int}{pnl_integration_GK2D}{\refstruct{PnlFunc} \ptr F, 
  double x0, double x1, double epsabs, double epsrel, double \ptr result, 
  double \ptr abserr,  int \ptr neval}
  \sshortdescribe This function is a synonymous of
  \reffun{pnl_integration_qng_2d} and is only available for backward
  compatibility. It is deprecated, please use \reffun{pnl_integration_qng_2d}
  instead. 

\item \describefun{int}{pnl_integration_qag}{\refstruct{PnlFunc} \ptr F,
  double x0, double x1, double epsabs, int limit, double epsrel, double \ptr
  result, double \ptr abserr,  int \ptr neval}
  \sshortdescribe Evaluates $\int_{x_0}^{x_1} F$ with an absolute error less
  than \var{espabs} and a relative error less than \var{esprel}. \var{x0} and
  \var{x1} can be non finite (i.e. \var{PNL_NEGINF} or \var{PNL_POSINF}). The
  value of the integral is stored in \var{result}, while the variables
  \var{abserr} and \var{neval} respectively contain the absolute error and the
  number of iterations. \var{limit} is the maximum number of subdivisions of the
  interval \var{(x0,x1)} used during the integration. If on input, \var{limit =
  0}, then 750 subdivisions are used.  This function returns \var{OK} if the
  required accuracy has been reached and \var{FAIL} otherwise. This function
  uses some adaptive procedures (qags and qagi routines from {\it QuadPack}).
  This function is able to handle functions \var{F} with integrable
  singularities on the interval \var{[x0,x1]}.

\item \describefun{int}{pnl_integration_qagp}{\refstruct{PnlFunc} \ptr F,
  double x0, double x1, const PnlVect \ptr singularities, double epsabs,
  int limit, double epsrel, double \ptr result, double \ptr abserr,  int \ptr neval}
  \sshortdescribe Evaluates $\int_{x_0}^{x_1} F$  for a function
  \var{F} with known singularities listed in \var{singularities}.
  \var{singularities} must be a sorted vector which does not contain \var{x0}
  nor \var{x1}.  \var{x0} and \var{x1} must be  finite. The value of the
  integral is stored in \var{result}, while the variables \var{abserr} and
  \var{neval} respectively contain the absolute error and the number of
  iterations. \var{limit} is the maximum number of subdivisions of the interval
  \var{(x0,x1)} used during the integration. If on input, \var{limit = 0}, then
  750 subdivisions are used.  This function returns \var{OK} if the required
  accuracy has been reached and \var{FAIL} otherwise. This function uses some
  adaptive procedures (qagp routine from {\it QuadPack}).  This function is
  able to handle functions \var{F} with integrable singularities on the interval
  \var{[x0,x1]}.
\end{itemize}


%% FFT function
\subsection{Fast Fourier Transform}
\subsubsection{Short Description}

In the case of Real Fourier transform, the Fourier coefficients satisfy the
following relation
\begin{equation}
  \label{eq:fft-sym}
  z_k = \overline{z_{N-k}}, 
\end{equation}
where $N$ is the number of discretization points.

A few remarks on the FFT of real functions and its inverse transformation :
\begin{itemize}
\item We only need half of the coefficients.
\item When a value is known to be real the imaginary part is not stored.
So the imaginary part of the zero-frequency component is never stored. It is
known to be zero.
\item For a sequence of even length the imaginary part of the frequency
  $n/2$ is not stored either, since the symmetry (\ref{eq:fft-sym}) implies
  that this is purely real too.
\end{itemize}


\paragraph{FFTPack storage}
\label{sec:fftpack-storage}

The functions use the fftpack storage convention for half-complex sequences.
In this convention, the half-complex transform of a real sequence is stored
with frequencies in increasing order, starting from zero, with the real and
imaginary parts of each frequency in neighboring locations.

The storage scheme is best shown by some examples. The table below shows the
output for an odd-length sequence, $n=5$.  The two columns give the
correspondence between the $5$ values in the half-complex sequence (stored in
a PnlVect $V$) and the values (PnlVectComplex $C$) that would be returned if
the same real input sequence were passed to pnl_dft_complex as a complex
sequence (with imaginary parts set to 0), 
\begin{equation}
  \begin{array}{l}
         C(0) =  V(0) + \imath 0, \\ 
         C(1) =  V(1) + \imath V(2), \\
         C(2) =  V(3) + \imath V(4), \\
         C(3) = V(3) - \imath V(4)=  \overline{C(2)} , \\
         C(4) = V(1) + \imath V(2)=  \overline{C(1)} 
  \end{array}   
\end{equation}

The elements of index greater than $N/2$ of the complex array, as $C(3)$
$C(4)$, are filled in using the symmetry condition.

The next table shows the output for an even-length sequence, $n=6$.
In the even case there are two values which are purely real, 
\begin{equation}
  \begin{array}{l}
         C(0) =  V(0) + \imath 0, \\ 
         C(1) =  V(1) + \imath V(2), \\
         C(2) =  V(3) + \imath V(4), \\
         C(3) = V(5) - \imath 0    =  \overline{C(0)} , \\
         C(4) = V(3) - \imath V(4) =  \overline{C(2)} , \\
         C(5) = V(1) + \imath V(2) =  \overline{C(1)} 
  \end{array}   
 \end{equation}


\subsubsection{Functions}

To use the following functions, you should include \verb!pnl_fft.h!.

The following functions comes from a C version of the Fortran FFTPack library
available on \url{http://www.netlib.org/fftpack}.
\begin{itemize}
\item \describefun{int}{pnl_fft_inplace}{\refstruct{PnlVectComplex} \ptr data}
  \sshortdescribe Computes the FFT of \var{data} in place. The original content
  of \var{data} is lost.

\item \describefun{int}{pnl_ifft_inplace}{\refstruct{PnlVectComplex} \ptr data}
  \sshortdescribe Computes the inverse FFT of \var{data} in place. The
  original content of \var{data} is lost.

\item \describefun{int}{pnl_fft}{const \refstruct{PnlVectComplex} \ptr in, 
    \refstruct{PnlVectComplex} \ptr out}
  \sshortdescribe Computes the FFT of \var{in} and stores it into \var{out}.

\item \describefun{int}{pnl_ifft}{const \refstruct{PnlVectComplex} \ptr in, 
    \refstruct{PnlVectComplex} \ptr out}
  \sshortdescribe Computes the inverse FFT of \var{in} and stores it into \var{out}.

\item \describefun{int}{pnl_fft2}{double \ptr re, double \ptr im, int n}
  \sshortdescribe Computes the FFT of the vector of length \var{n} whose real
  (resp. imaginary) parts are given by the arrays \var{re}
  (resp. \var{im}). The real and imaginary parts of the FFT are respectively
  stored in \var{re} and \var{im} on output.

\item \describefun{int}{pnl_ifft2}{double \ptr re, double \ptr im, int n}
  \sshortdescribe Computes the inverse FFT of the vector of length \var{n}
  whose real (resp. imaginary) parts are given by the arrays \var{re}
  (resp. \var{im}). The real and imaginary parts of the inverse FFT are
  respectively stored in \var{re} and \var{im} on output.

\item \describefun{int}{pnl_real_fft}{const \refstruct{PnlVect} \ptr in, 
    \refstruct{PnlVectComplex} \ptr out}
  \sshortdescribe Computes the FFT of the real valued sequence \var{in} and
  stores it into \var{out}.

\item \describefun{int}{pnl_real_ifft}{const \refstruct{PnlVect} \ptr in, 
    \refstruct{PnlVectComplex} \ptr out}
  \sshortdescribe Computes the inverse FFT of \var{in} and stores it into \var{out}.

\item \describefun{int}{pnl_real_fft_inplace}{double \ptr data, int n}
  \sshortdescribe Computes the FFT of the real valued vector \var{data} of
  length \var{n}. The result is stored in \var{data} using the FFTPack storage
  described above, see~\ref{sec:fftpack-storage}.

\item \describefun{int}{pnl_real_ifft_inplace}{double \ptr data, int n}
  \sshortdescribe Computes the inverse FFT of the vector \var{data} of length
  \var{n}. \var{data} is supposed to be the FFT coefficients a real valued
  sequence stored using the FFTPack storage. On output, \var{data} contains
  the inverse FFT.

\item \describefun{int}{pnl_real_fft2}{double \ptr re, double \ptr im, int n}
  \sshortdescribe Computes the FFT of the real vector \var{re} of length \var{n}.
  \var{im} is only used on output to store the imaginary part the FFT. The
  real part is stored into \var{re}
  
\item \describefun{int}{pnl_real_ifft2}{double \ptr re, double \ptr im, int n}
  \sshortdescribe Computes the inverse FFT of the vector \var{re + i * im} of
  length \var{n}, which is supposed to be the FFT of a real valued
  sequence. On exit, \var{im} is unused. 
\end{itemize}

%% Laplace transform
\subsection{Inverse Laplace Transform}
\subsubsection{Short Description}

For a real valued function $f$ such that $t \longmapsto f(t) \expp{- \sigma_c
  t}$ is integrable over $\R^+$, we can define its Laplace transform
\begin{equation*}
  \hat{f}(\lambda) = \int_0^\infty f(t) \expp{- \lambda t} dt \qquad
  \mbox{for $\lambda \in \C$ with $\real{\lambda} \ge \sigma_c$}.
\end{equation*}

\subsubsection{Functions}

To use the following functions, you should include \verb!pnl_laplace.h!.

\begin{itemize}
\item \describefun{double}{pnl_ilap_euler}{\refstruct{PnlCmplxFunc}
    \ptr fhat, double t, int N, int M}
  \sshortdescribe Computes $f(\var{t})$ where $f$ is given by its Laplace
  transform \var{fhat} by numerically inverting the Laplace transform using
  Euler's summation. The values \var{N = M = 15} usually give a very good
  accuracy. For more details on the accuracy of the method. 

\item \describefun{double}{pnl_ilap_cdf_euler}{\refstruct{PnlCmplxFunc}
  \ptr fhat, double t, double h, int N, int M}
  \sshortdescribe Computes the cumulative distribution function $F(\var{t})$
  where $F(x) = \int_0^x f(t) dt$ and $f$ is a density function with values on
  the positive real linegiven by its Laplace transform \var{fhat}. The
  computation is carried out by numerical inversion of the Laplace transform
  using Euler's summation. The values \var{N = M = 15} usually give a very
  good accuracy. The parameter \var{h} is the discretization step, the
  algorithm is very sensitive to the choice of \var{h}.

\item \describefun{double}{pnl_ilap_fft}{\refstruct{PnlVect} \ptr res,
    \refstruct{PnlCmplxFunc} \ptr fhat, double T, double eps}
  \sshortdescribe Computes $f(t)$ for $t \in [h, \var{T}]$ on a regular grid
  and stores the values in \var{res}, where $h = T / {\mathrm size}(res)$. The
  function $f$ is defined by its Laplace transform \var{fhat}, which is
  numerically inverted using a Fast Fourier Transform algorithm. The size of
  \var{res} is related to the choice of the relative precision \var{eps}
  required on the value of $f(t)$ for all $t \le T$.

\item \describefun{double}{pnl_ilap_gs}{\refstruct{PnlFunc} \ptr fhat, double
    t, int n}
  \sshortdescribe Computes $f(\var{t})$ where $f$ is given by its Laplace
  transform \var{fhat} by numerically inverting the Laplace transform using a
  weighted combination of different Gaver Stehfest's algorithms. Note that
  this function does not need the complex valued Laplace transform but only the
  real valued one. \var{n} is the number of terms used in the weighted combination.

\item \describefun{double}{pnl_ilap_gs_basic}{\refstruct{PnlFunc}
    \ptr fhat, double t, int n}
  \sshortdescribe Computes $f(\var{t})$ where $f$ is given by its Laplace
  transform \var{fhat} by numerically inverting the Laplace transform using
  Gaver Stehfest's method. Note that this function does not
  need the comple valued Laplace transform but only the real valued
  one. \var{n} is the number of iterations of the algorithm.
  {\bf Note : }~This function is provided only for test purposes, even though
  the function \reffun{pnl_ilap_gs} gives far more accurate results.
\end{itemize}

\subsection{Nonlinear Constrained Optimization}
\subsubsection{Short Description}

A standard Constrained Nonlinear Optimization problem can be written as:

\begin{equation*}
(O)\;
\left\{
\begin{tabular}{l}
    $\displaystyle   \min \; f(x)$ \\
    $\displaystyle c^I(x) \geq 0$ \\
    $\displaystyle c^E(x) = 0$
\end{tabular}
\right.
\end{equation*}

where the function $f : \mathbb{R}^n \rightarrow  \mathbb{R}$ is the objective function, $c^I : \mathbb{R}^n \rightarrow  \mathbb{R}^{m_I} $ are the inequality constraints and $c^E : \mathbb{R}^n \rightarrow  \mathbb{R}^{m_E} $ are the equality constraints. These functions are supposed to be smooth.

In general, the inequality constraints are of the form $c^I(x) = \left (g(x), \; x-l, \; u-x \right )$. The vector $l$ and $u$ are the lower and upper bounds on the variables $x$ and $g(x)$ and the non linear inequality constraints.

Under some conditions, if $x \in \mathbb{R}^n$ is a solution of problem ($O$), then there exist a vector $\lambda=(\lambda^I,\lambda^E) \in \mathbb{R}^{m_I} \times \mathbb{R}^{m_E}$, such that the well known Karush-Kuhn-Tucker (KKT) optimality conditions are satisfied:

    \begin{equation*}
    (P)\;
    \left\{
    \begin{tabular}{c}
    $\nabla \ell(x,\lambda^I, \lambda^E) = \nabla f(x) - \nabla c^I(x) \lambda^I - \nabla c^E(x) \lambda^E= 0$ \\
    $c^E(x) = 0 $ \\
    $c^I(x) \geq 0 $ \\
    $\lambda^I \geq 0 $\\
    $c^I_i(x) \lambda^I_i =0, \; i=1...m_I$ \\
    \end{tabular}
    \right.
    \end{equation*}

$l$ is known as the lagrangian of the problem $(O)$, $\lambda^I$ and $\lambda^E$ as the dual variables while $x$ is the primal variable.

\subsubsection{Functions}

To use the following functions, you should include \verb!pnl_optim.h!.

To solve an inequality constrained optimization problem, ie $m_E=0$, we provide the following function.
\begin{itemize}
\item \describefun{int}{pnl_optim_intpoints_bfgs_solve}{\refstruct{PnlRnFuncR}\ptr func, \refstruct{PnlRnFuncRm}\ptr grad_func, \refstruct{PnlRnFuncRm}\ptr nl_constraints, \refstruct{PnlVect}\ptr lower_bounds, \refstruct{PnlVect}\ptr upper_bounds, \refstruct{PnlVect}\ptr x_input, double tolerance, int iter_max, int print_inner_steps, \refstruct{PnlVect}\ptr output}
  \sshortdescribe This function has the following arguments:

\begin{itemize}
 \item \var{func} is the function to minimize $f$.
 \item \var{grad} is the gradient of $f$. If this gradient is not available, then enter \var{grad}=NULL. In this case, finite difference will be used to estimate the gradient.
 \item \var{nl_constraints} is the function $g(x)$, ie the non linear inequality constraints.
 \item \var{lower_bounds} are the lower bounds on $x$.
 \item \var{upper_bounds} are the upper bounds on $x$.
 \item \var{x_input} is the initial point where the algorithm starts.
 \item \var{tolerance} is the precision required in solving (P).
 \item \var{iter_max} is the maximum number of iterations in the algorithm.
 \item \var{print_algo_steps} is a flag to decide to print information.
 \item \var{x_output} is the point where the algorithm stops.
\end{itemize}

The algorithm returns an $int$, its value depends on the output status of the algorithm. We have 4 cases:

\begin{itemize}
 \item 0: Failure: Initial point is not strictly feasible.
 \item 1: Step too small, we stop the algorithm.
 \item 2: Maximum iteration reached.
 \item 3: A solution has been found up to the required accuracy.
\end{itemize}

The last case is equivalent to the two inequalities:

$$ || \nabla \ell(x,\lambda^I)||_{\infty} = ||\nabla f(x) - \nabla c^I(x) \lambda^I ||_{\infty} < \var{tolerance} $$
$$ || c^I(x) \lambda^I ||_{\infty} < \var{tolerance} $$

where $c^I(x) \lambda^I$ is a vector of term by term multiplication of $c^I(x)$ and $\lambda^I$.

The first inequality is known as the optimality condition, the second one as the complementarity condition.\\

\textbf{Important Remark 1}: The algorithm we implement requires that the initial point $x_0$, given as an input to the algorithm, to be strictly feasible, ie: $c(x_0)>0$.\\
\textbf{Important Remark 2}: The algorithm try to find a pair ($x$, $\lambda$) that solves the equations ($P$), but this does not guarantee that $x$ is a global minimum of $f$ on the set $\{c(x)\geq0\}$.

\end{itemize}

